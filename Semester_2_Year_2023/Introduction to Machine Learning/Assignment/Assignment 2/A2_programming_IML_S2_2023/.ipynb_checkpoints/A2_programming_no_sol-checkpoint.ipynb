{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP3670/6670 Programming Assignment 2 - Clustering and Regularization\n",
    "---\n",
    "\n",
    "**Enter Your Student ID:**\n",
    "\n",
    "**Your Name:**\n",
    "    \n",
    "**Deadline:** \n",
    "\n",
    "**Submit:** Write your answers in this file, and submit a single Jupyter Notebook file (.ipynb) on Wattle. Rename this file with your student number as 'uXXXXXXX.ipynb'. Note: you don't need to submit the .png or .npy files. \n",
    "\n",
    "**Enter Discussion Partner IDs Below:**\n",
    "You could add more IDs with the same markdown format above.\n",
    "\n",
    "**Programming Section**:\n",
    "- Tasks 1.1 - 1.4: 30%\n",
    "- Task 1.5-1.7: 30%\n",
    "- Task 2: 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (1.22.4)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from matplotlib) (4.28.5)\n",
      "Requirement already satisfied: six in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: patchify in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/ciuca/miniconda/envs/science/lib/python3.9/site-packages (from patchify) (1.22.4)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "! pip install numpy\n",
    "import numpy as np\n",
    "! pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "! pip install patchify\n",
    "from patchify import patchify\n",
    "import math as m\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Clustering\n",
    "-----------\n",
    "These programming exercises will focus on K-means clustering. \n",
    "\n",
    "If you're unsure of how K-Means works, read this very helpful and freely available online breakdown from Stanford's CS221 course; https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "\n",
    "This assignment requires you to loosely interpret how K-Means is a specific case of a more general algorithm named Expectation Maximisation. This is explained toward the end of the above article.\n",
    "\n",
    "First, let us load the dataset we'll be using throughout Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.load(\"./data_clustering.npy\")\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is a special, simple case of the Expectation Maximisation (EM) algorithm.\n",
    "\n",
    "This simplified EM (K-Means), is divided into two steps.\n",
    "\n",
    "We refer to a cluster representative as the \"centroid\".\n",
    "\n",
    "The **E-Step**, where for every datapoint in your dataset, you find which \"centroid\" is closest to it, and record that information.\n",
    "\n",
    "The **M-Step**, where you move each \"centroid\" to the center of the samples which were found to be closest to it in the **E-Step**.\n",
    "\n",
    "Each *centroid* is simply an estimated mean of a cluster. If you have $1$ centroid, then this centroid will become the mean of all your data.\n",
    "\n",
    "Centroids are initially random values, and the K-Means algorithm attempts to modify them so that each one represents the center of a cluster.\n",
    "\n",
    "We have implemented a centroids initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55638768  1.19083041]\n",
      " [ 0.99468733 -0.63105385]\n",
      " [-0.80861347 -0.47487527]\n",
      " [ 0.83443335  0.7038998 ]]\n"
     ]
    }
   ],
   "source": [
    "def initialise_parameters(m, X):\n",
    "    C = X[np.random.choice(X.shape[0], m)]\n",
    "    return C\n",
    "\n",
    "C = initialise_parameters(4, X)\n",
    "print(C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the K-Means algorithm.\n",
    "\n",
    "---\n",
    "   **TASK 1.1:** Create a function $E\\_step(C, X) = L$, where $L$ is a matrix of the same dimension of the dataset $X$.\n",
    "   \n",
    "   This function is is the **E-Step** (or \"assignment step\") mentioned earlier.\n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** \n",
    "- https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "- https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm\n",
    "- Each row of $L$ is a centroid taken from $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(C, X):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    \n",
    "L = E_step(C, X)\n",
    "plt.scatter(L[:, 0], L[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TASK 1.2:** Create a function $M\\_step(C, X, L) = C$ which returns $C$ modified so that each centroid in $C$ is placed in the middle of the samples assigned to it. This is the **M-Step**.\n",
    "\n",
    "In other words, make each centroid in $C$ the average of all the samples which were found to be closest to it during the **E-step**. This is also called the \"update step\" for K-Means.\n",
    "\n",
    "---\n",
    "\n",
    "**HINT:** https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_equal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(C, X, L):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "print('Before:')\n",
    "print(C)\n",
    "print('\\nAfter:')\n",
    "new_C = M_step(C, X, L)\n",
    "print(new_C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TASK 1.3:** Implement $kmeans(X, m, threshold) = C, L$ which takes a dataset $X$ (of any dimension) and a scalar value $m$ and a scalar $threshold$ as input. This function uses the previous 3 functions to:\n",
    "- generate $m$ centroids.\n",
    "- iterate between the E and M steps until the difference of loss values between two iterations is less than the threshold to classify the $m$ clusters.\n",
    "\n",
    "...and then returns:\n",
    "- $C$, the centers of the $m$ clusters after convergence.\n",
    "- $L$, the labels (centroid vectors) assigned to each sample in the dataset after convergence.\n",
    "---\n",
    "**HINT:** Using initialise_parameters to initial centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, m, threshold):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "#CODE TO DISPLAY YOUR RESULTS. DO NOT MODIFY.\n",
    "C_final, L_final = kmeans(X, 4, 1e-6)\n",
    "print('Initial Parameters:')\n",
    "print(C)\n",
    "print('\\nFinal Parameters:')\n",
    "print(C_final)\n",
    "\n",
    "def allocator(X, L, c):\n",
    "    cluster = []\n",
    "    for i in range(L.shape[0]):\n",
    "        if np.array_equal(L[i, :], c):\n",
    "            cluster.append(X[i, :])\n",
    "    return np.asarray(cluster)\n",
    "\n",
    "colours = ['r', 'g', 'b', 'y']\n",
    "for i in range(4):\n",
    "    cluster = allocator(X, L_final, C_final[i, :])\n",
    "    plt.scatter(cluster[:,0], cluster[:,1], c=colours[i])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer should like this, maybe with different colors:\n",
    "![image](./cluster.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 1.4:**\n",
    "Implement a function `add_noise` to explore how the results of your clustering method change when you add Gaussian noise to your data.\n",
    "This function will take two parameters, your dataset $X$, and the noise level you want to add.\n",
    "\n",
    "Investigate how adding noise affects the robustness of your K-Means clustering by visually inspecting the results you get after applying K-means to the noisy data.\n",
    "\n",
    "---\n",
    "\n",
    "**Hint:** You can use the function np.random.normal to add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add noise to the data\n",
    "def add_noise(X, noise_level):\n",
    "   #YOUR CODE Here\n",
    "\n",
    "   pass\n",
    "\n",
    "# Running K-Means with different noise levels\n",
    "noise_levels = [0.01, 0.05, 0.1]\n",
    "for noise_level in noise_levels:\n",
    "    noisy_data = add_noise(X, noise_level)\n",
    "    #YOUR CODE Here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 1.5:**\n",
    "Friends-of-Friends (FoF) Algorithm\n",
    "\n",
    " In this task, you will implement and apply the Friends-of-Friends (FoF) clustering algorithm to identify clusters in your dataset $X$. \n",
    " \n",
    " FoF is used to identify groups of particles in cosmological simulations that describe how the Universe evolves. For example, you can have a look at the incredible IllustrisTNG simulations https://www.tng-project.org/media/. The videos show the hierarchical structure formation in a simulated Universe.\n",
    "\n",
    " Implementation details:\n",
    " \n",
    "The goal of the FoF algorithm is to identify clusters of objects that are closer to each other than a specified distance threshold.\n",
    "\n",
    "If two objects are closer to each other than a given distance (called the linking length and denoted by $b$), they are considered \"friends.\" If object A is friends with object B, and object B is friends with object C, then A, B, and C are all part of the same cluster.\n",
    "\n",
    "General algorithm steps:\n",
    "1. **Initialization**: Initialise a set of points.\n",
    "2. **Linking Length Definition**: Select a distance, $b$, to be the criterion for connecting entities. \n",
    "3. **Clustering**:\n",
    "\n",
    "   a. **Connection Analysis**: Determine connections between entities within distance $b$. If the distance between entitities A and B is less than $b$, then A and B are connected.\n",
    "   \n",
    "   b. **Group Formation**: Form or join groups based on connections; merge groups if necessary. \n",
    "\n",
    "4. **Completion**: Finalize groups; unconnected entities may be disregarded.\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    " **Hint**: You can use the  distance_matrix from scipy.spatial to compute the distances between the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.23606798, 3.        ],\n",
       "       [2.23606798, 1.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example showing how to use the distance_matrix function\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "#The distance matrix is a matrix containing the Euclidean distances between all pairs of points of a given set of points.\n",
    "arr1 = np.array([[1, 1], [2, 4]])\n",
    "arr2 = np.array([[3, 2], [1, 4]])\n",
    "\n",
    "dist_matrix = distance_matrix(arr1, arr2)\n",
    "dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 4\n"
     ]
    }
   ],
   "source": [
    "#implement your function here\n",
    "#Your fof_clustering(X, b)  should return a list of lists, where each list contains the indices of the points in a cluster.\n",
    "def fof_clustering(X, b):\n",
    "    # YOUR CODE HERE\n",
    "        \n",
    "    pass\n",
    "\n",
    "# Example usage\n",
    "b = 0.4\n",
    "\n",
    "clusters_fof = fof_clustering(X, b)\n",
    "print(\"Number of clusters found:\", len(clusters_fof))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot your FoF clustering results here\n",
    "\n",
    "colors = plt.cm.get_cmap('viridis', len(clusters_fof))\n",
    "\n",
    "for i, cluster in enumerate(clusters_fof):\n",
    "    plt.scatter(X[cluster, 0], X[cluster, 1], color=colors(i), label=f'Cluster {i+1}')\n",
    "\n",
    "plt.title('Friends-of-Friends Clusters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TASK 1.6:** \n",
    "The Within-Cluster-Sum-of-Squares (WCSS) is defined as:\n",
    "\n",
    "$$\\text{WCSS} = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\left\\| \\mathbf{x}_i - \\mathbf{c}_k \\right\\|^2$$\n",
    "\n",
    "Create a function `wcss` which takes as input the data and the clusters and computes the total within-cluster sum of square.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wcss(X, clusters):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate what value of b is best.\n",
    "b_values = np.arange(0.1, 1, 0.05)\n",
    "wcss_values = [wcss(X, fof_clustering(X, b)) for b in b_values]\n",
    "\n",
    "plt.plot(b_values, wcss_values, marker='o')\n",
    "plt.xlabel('Linking Length (b)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method for Optimal b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TASK 1.7:**  \n",
    "Compare the robustness of the FoF (Friends-of-Friends) algorithm on the dataset X by investigating how the WCSS (within-cluster sum of squares) changes depending on the level of noise added to the data.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Selection & Regularization\n",
    "---\n",
    "\n",
    "For excercise 2, we are going to to attempt fitting a model to a dataset.\n",
    "Our goal is to understand the balance between model's capacity and the quality and quantity of the data that it learns from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot the graph of sin(x) and sample 20 points from the curve with noise\n",
    "X = np.linspace(-np.pi, np.pi, 256, endpoint=True)\n",
    "def f(x):\n",
    "    return np.sin(x*np.pi/1.5) * x\n",
    "y = f(X)\n",
    "\n",
    "epsilon = 0.5\n",
    "number_samples = 15\n",
    "\n",
    "# Sample 20 points from the curve with noise\n",
    "X_sample = np.linspace(-np.pi, np.pi, number_samples)\n",
    "y_sample = f(X_sample) + epsilon * np.random.randn(number_samples)\n",
    "\n",
    "# Plot the graph\n",
    "plt.plot(X, y, color=\"blue\", linewidth=0.5, linestyle=\"--\", label=\"sin(x)\")\n",
    "plt.scatter(X_sample, y_sample, color=\"red\", label=\"sampled\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curve fitting is one of the most fundamental machine learning tasks. Of which linear regression is the most straight forward. It establishes a relationship between two variables using a linear equation. When predicting one variable based on another, we call the variable we're predicting the dependent variable, and the variable we use for prediction the independent variable.\n",
    "\n",
    "In this task, we're aiming to predict y (the dependent variable) based on x (the independent variable). The function we're trying to learn is:\n",
    "\n",
    "$$ \n",
    "    y = sin (2 \\pi x)\n",
    "$$\n",
    "\n",
    "But all we have is a collection of data points sampled from this function with some noise added to it. Our goal is to find a function that best fits the data points. This is called curve fitting.\n",
    "\n",
    "Objective:\n",
    "\n",
    "Your goal is to determine the best linear relationship (a straight line) that describes how changes in the independent variable x can predict changes in the dependent variable $y$.\n",
    "\n",
    "The Linear Equation:\n",
    "\n",
    "The equation for this line is:\n",
    "$$ \n",
    "    y=wx+b \n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $y$ is the predicted output,\n",
    "- $x$ is the input feature,\n",
    "- $w$ is the weight (or slope) of the line, and\n",
    "- $b$ is the bias (or y-intercept) of the line.\n",
    "\n",
    "The terms \"weight\" and \"bias\" stem from machine learning terminology, but you might have referred to them as the \"coefficient\" and the \"intercept\", respectively.\n",
    "\n",
    "Learning:\n",
    "\n",
    "\"Learning\" in the context of linear regression means adjusting values of w and \n",
    "b based on the data (i.e., the given $x$ and $y$ values) such that the line (or the \"model\") predicts the output \n",
    "$y$ as accurately as possible for each corresponding input $x$.\n",
    "\n",
    "\n",
    "When designing a linear regression model, our primary goal is to find a line that best fits the given data points. This \"best fit\" can be thought of as the line that minimizes the distance (error) between itself and each data point. Specifically, we want to minimize the sum of the squared vertical distances (or residuals) between the data points and the line. Let's consider our line as $y=wx+b$. For each data point $(x_i, y_i)$, the predicted value on the line is $\\hat{y}^i ​= wx_i + b$. The residual (difference between observed and predicted values) for this data point is $y_i ​ − \\hat{y}^i$. We square these residuals to penalize larger deviations more and sum them up for all data points. We can express this as a function of $w$ and $b$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 = \\sum_{i=1}^{n} (y_i - (wx_i + b))^2\t\n",
    "$$\n",
    "\n",
    "Differentiating and setting the derivatives for $w$ and $b$ to zero yields the normal equations:\n",
    "$$\n",
    "w = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - w\\bar{x}\n",
    "$$\n",
    "\n",
    "where \n",
    "$\\bar{x}$\n",
    "  and \n",
    "$\\bar{y}$\n",
    "​\n",
    "  are the means of $x$ and $y$ values respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "   **TASK 2.1:** \n",
    "Find an expression for the values of $w$ and $b$ using the data provide and plot the resulting function. You can use the numpy library to compute the mean of the $x$ and $y$ values.\n",
    "\n",
    "Plot the data points and overlay the linear regression line on top of the data.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the optimal weight and bias terms for the data X_sample, y_sample\n",
    "\n",
    "x_mean = np.mean(X_sample)\n",
    "y_mean = np.mean(y_sample)\n",
    "\n",
    "weight = #YOUR CODE HERE\n",
    "bias = #YOUR CODE HERE\n",
    "\n",
    "print(\"Optimal weight: \", weight)\n",
    "print(\"Optimal bias: \", bias)\n",
    "\n",
    "# Plot the data and the optimal linear regression line\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, through this exercise you can see that while this line does capture the rough trend of the data, it is not a very good fit. This is because the data is not linear. If we want to improve our fit, we can, instead of trying to fit a linear function, try to fit a higher degree polynomial function. Although the target function may not be a polynomial either, we can still use a polynomial to approximate it and because with higher degree polynomials we can get more complex curves, we can get a better fit. Let's try to fit a 2nd degree polynomial (Quadratic) and a 4th degree polynomial (Quartic) to the data.\n",
    "\n",
    "In general, a polynomial of degree n has the form:\n",
    "$$\n",
    "f(x) = a_0 + a_1x + a_2x^2 + a_3x^3 + ... + a_nx^n\n",
    "$$\n",
    "where $w_0, w_1, w_2, ..., w_n$ are the coefficients of the polynomial.\n",
    "\n",
    "We can combine these coefficients into a single weight vector $\\mathbf{\\hat{w}} = [w_0, w_1, \\dots, w_n]^T$ (For the linear polynomial above we would have had $\\mathbf{\\hat{w}} = [b, w]^T$). \n",
    "\n",
    "As a trick to help us in our calculation, we can create the matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ ($m$ is the # of data points and $n$ is the degree of the fitting polynomial), which is called the Vandermonde matrix. This matrix is defined as: \n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} \n",
    "1 & x_1 & x_1^2 & \\dots & x_1^n \\\\\n",
    "1 & x_2 & x_2^2 & \\dots & x_2^n \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_m & x_m^2 & \\dots & x_m^n \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We define $\\mathbf{A}$ this way so that we can write the polynomial as a matrix multiplication:\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{\\hat{w}} = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\dots & x_1^n \\\\\n",
    "1 & x_2 & x_2^2 & \\dots & x_2^n \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_m & x_m^2 & \\dots & x_m^n \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "w_0 + w_1x_1 + w_2x_1^2 + \\dots + w_nx_1^n \\\\\n",
    "w_0 + w_1x_2 + w_2x_2^2 + \\dots + w_nx_2^n \\\\\n",
    "\\vdots \\\\\n",
    "w_0 + w_1x_m + w_2x_m^2 + \\dots + w_nx_m^n \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It follows, that the following difference, $\\mathbf{A}\\mathbf{\\hat{w}} - \\mathbf{y}$ has the property the entry $i$ of this vector is the signed difference between the predicted and true value for the $i$'th data point. Our goal is to minimize the squared error of this, or, $E(\\mathbf{\\hat{w}}) = \\| \\mathbf{A}\\mathbf{\\hat{w}} - \\mathbf{y} \\|^2 $. \n",
    "\n",
    "Expanding this out, we get:\n",
    "\n",
    "$$ \n",
    "    E(\\mathbf{\\hat{w}}) = (\\mathbf{A}\\mathbf{\\hat{w}} - \\mathbf{y})^T (\\mathbf{A}\\mathbf{\\hat{w}} - \\mathbf{y}) \n",
    "$$\n",
    "\n",
    "To minimize this error, we'll set its gradient with respect to $\\left( \\mathbf{w} \\right)$ to zero:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\nabla E(\\mathbf{w}) = \\nabla (\\mathbf{A}\\mathbf{w} - \\mathbf{y})^T (\\mathbf{A}\\mathbf{w} - \\mathbf{y}) = 2\\mathbf{A}^T\\mathbf{A}\\mathbf{w} - 2\\mathbf{A}^T\\mathbf{y} = 0\n",
    "$$\n",
    "\n",
    "\n",
    "So we have, \n",
    "$$\n",
    "    2\\mathbf{A}^T\\mathbf{A}\\mathbf{w} - 2\\mathbf{A}^T\\mathbf{y} = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "   **TASK 2.2:** \n",
    "\n",
    "Your task is to write a function `calculate_weights`, which takes in\n",
    "- An $m$ dimensional vector of data points $X$\n",
    "- An $m$ dimensional vector of corresponding values $y$ obtained by sampling from a noisy function\n",
    "- An integer $n$ representing the degree of the polynomial to fit to the data\n",
    "\n",
    "and returns the weight vector $\\mathbf{\\hat{w}}$ that minimizes the squared error.\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: You can use `np.linalg.inv` to invert a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m dimentional data vector $X$\n",
    "# m dimentional data vector $y$\n",
    "# integer $n$ - degree of polynomial to fit\n",
    "def calculate_weights(X, y, n):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    return w\n",
    "    \n",
    "def poly_vector(x, n):\n",
    "    return np.array([x ** i for i in range(n+1)])\n",
    "    \n",
    "def plot_poly_fit(X_sample, y_sample, w):\n",
    "    n = len(w) - 1 # degree of polynomial\n",
    "\n",
    "    # Plot the data and the optimal linear regression line\n",
    "    plt.title(f'{n} degree polynomial')\n",
    "    plt.scatter(X_sample, y_sample, label='Data')\n",
    "    plt.plot(X, y, color=\"blue\", linewidth=0.5, linestyle=\"--\", label=\"sin(x)\")\n",
    "    plt.plot(X, w @ poly_vector(X, n), 'r', label='Model')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(-3.5, 3.5)\n",
    "    plt.ylim(-2.75, 1.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test for quadratic polynomial\n",
    "w = calculate_weights(X_sample, y_sample, 2)\n",
    "plot_poly_fit(X_sample, y_sample, w)\n",
    "\n",
    "# Test for quadratic polynomial\n",
    "w = calculate_weights(X_sample, y_sample, 4)\n",
    "plot_poly_fit(X_sample, y_sample, w)\n",
    "\n",
    "# Test for 6th degree polynomial\n",
    "w = calculate_weights(X_sample, y_sample, 6)\n",
    "plot_poly_fit(X_sample, y_sample, w)\n",
    "\n",
    "# Test for 13th degree polynomial\n",
    "w = calculate_weights(X_sample, y_sample, 13)\n",
    "plot_poly_fit(X_sample, y_sample, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that initially, as we increase the degree of the polynomial, we get a better fit to the data. However, as we continue to increase the polynomial degree even further, the model not only fits the training data but also starts capturing the noise in the data. This phenomenon can be understood better using the concept of the bias-variance trade-off.\n",
    "\n",
    "- Bias: Bias refers to the error due to overly simplistic assumptions in the learning algorithm. In our context, using a low-degree polynomial might be too simple to capture the underlying structure of the data. Such a model will have high bias because it assumes the data can be represented by a simple curve, even when it cannot. As a result, it systematically misrepresents the data, leading to consistent and high error on both the training and test datasets.\n",
    "\n",
    "- Variance: Variance refers to the error due to too much complexity in the learning algorithm. With a high-degree polynomial, our model becomes extremely flexible. Such flexibility allows the model to fit the training data very closely, including its noise and fluctuations. However, this results in a model with high variance, as even slight changes in the training data can lead to significantly different models. These models perform well on the training data but fail to generalize to new, unseen data, leading to high error on the test dataset.\n",
    "\n",
    "- Bias-Variance Trade-off: Increasing the degree decreases bias since the model becomes more flexible and can fit the training data better.\n",
    "However, past a certain point, increasing the degree also increases variance since the model begins to fit the noise in the training data.\n",
    "The bias-variance trade-off is the balance between these two sources of error. Ideally, we want to find the right level of model complexity that captures the underlying structure of the data but doesn't get swayed by its noise. \n",
    "\n",
    "\n",
    "If we sample another set of points from the same distribution, which we will call the test set, we can get a better picture of how well our model generalizes to unseen data. We can visualize the training and test set together to compare them.\n",
    "\n",
    "\n",
    "---\n",
    "   **TASK 2.3:** \n",
    "\n",
    "Write an error function `polynomial_regression_error` which computes the mean squared error between the predicted outputs and the true outputs for any input data. Then use this function to find the training and test errors for each of the models we have trained so far. Plot these errors as a function of the degree of the polynomial.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_testing_points = 15\n",
    "\n",
    "# Sample points from the curve with noise\n",
    "X_sample_testing = np.linspace(-np.pi, np.pi, number_testing_points)\n",
    "y_sample_testing = f(X_sample_testing) + epsilon * np.random.randn(number_testing_points)\n",
    "\n",
    "# Given an inputs:\n",
    "#   X_training_values - a vector of training inputs\n",
    "#   y_training_values - a vector of training outputs\n",
    "#   X_testing_values - a vector of testing inputs\n",
    "#   y_testing_values - a vector of testing outputs\n",
    "#   n - the degree of the polynomial to fit\n",
    "# Return:\n",
    "#   y_predicted_training_error - the training error\n",
    "#   y_predicted_testing_error - the testing error\n",
    "\n",
    "def polynomial_regression_error(X_training_values, y_training_values, X_testing_values, y_testing_values, n):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    return y_predicted_training_error, y_predicted_testing_error\n",
    "\n",
    "\n",
    "training_error = []\n",
    "testing_error = []\n",
    "for n in range(1, 20):\n",
    "    y_predicted_training_error, y_predicted_testing_error = polynomial_regression_error(X_sample, y_sample, X_sample_testing, y_sample_testing, n)\n",
    "    training_error.append(y_predicted_training_error)\n",
    "    testing_error.append(y_predicted_testing_error)\n",
    "\n",
    "# Plot the training and testing error\n",
    "#YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see clearly that as the degree of the polynomial increases, the model fits the training data better and better. However, the model also becomes more complex, and starts to fit the noise in the data as well. The result is that the model starts to overfit the data, and will actually perform worse on the unseen test data. This is a classic example of the bias-variance tradeoff in machine learning and is not unique to polynomial regression.\n",
    "\n",
    "**How do we overcome overfitting?**\n",
    "\n",
    "This is where regularization comes into play. Regularization techniques add a penalty to the cost function to constrain or shrink the model parameters, discouraging overly complex models which can overfit the training data. By adding this penalty, regularization ensures that the model remains somewhat simpler, even if we are using a high-degree polynomial or a complicated neural network. There are different forms of regularization, but the most common are L1 regularization (which tends to produce sparse weight vectors) and L2 regularization (which tends to produce smaller weight vectors). In polynomial regression, L2 regularization (also known as Ridge regression) is commonly used.\n",
    "\n",
    "\n",
    "So what does this mean for our polynomial regression model? We can add a regularization term to the cost function:\n",
    "$$\n",
    "    E(\\mathbf{w}) = \\| \\mathbf{A}\\mathbf{w} - \\mathbf{y} \\|^2 + \\lambda \\| \\mathbf{w} \\|^2\n",
    "$$\n",
    "\n",
    "Differentiating this with respect to $\\left( \\mathbf{w} \\right)$ gives:\n",
    "$$\n",
    "    \\nabla E(\\mathbf{w}) = 2\\mathbf{A}^T\\mathbf{A}\\mathbf{w} - 2\\mathbf{A}^T\\mathbf{y} + 2\\lambda\\mathbf{w}\n",
    "$$\n",
    "\n",
    "We can see that with greater $\\lambda$, the model will be penalized more for having large weights. This will encourage the model to be simpler, and will reduce overfitting. \n",
    "\n",
    "---\n",
    "   **TASK 2.4:** \n",
    "Your task is to implement polynomial regression with L2 regularization. To do this you will need to write a function `calculate_weights_regularized` that takes in the training data, the test data, the degree of the polynomial, and the regularization parameter $\\lambda$, and returns the polynomial coefficients $\\mathbf{w}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs :\n",
    "# - X_sample: A vector of m data points\n",
    "# - y_sample: A vector of m data points, obtained by evaluating the function f at each x in X with some added noise\n",
    "# - lambda: A regularization parameter\n",
    "# - n: The degree of the polynomial to be used in the regression\n",
    "def calculate_weights_regularized(X_sample, y_sample, n, lam):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    return w\n",
    "\n",
    "# Test for 12 degree, and lambda = 0.1\n",
    "w = calculate_weights_regularized(X_sample, y_sample, 12, 0)\n",
    "plot_poly_fit(X_sample, y_sample, w)\n",
    "\n",
    "# Test for 12 degree, and lambda = 1\n",
    "w = calculate_weights_regularized(X_sample, y_sample, 12, 1)\n",
    "plot_poly_fit(X_sample, y_sample, w)\n",
    "\n",
    "# Test for 12 degree, and lambda = 5\n",
    "w = calculate_weights_regularized(X_sample, y_sample, 12, 5)\n",
    "plot_poly_fit(X_sample, y_sample, w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "   **TASK 2.5:** \n",
    "We can see that with a larger regularization paramater $\\lambda$ we get a smoother fit as the coefficients are penalized more for being large. Write a function `polynomial_regression_error_regularized` that computes the training and test error as a function of $\\lambda$ and plot the result for a 12 degree polynomial fit to see how the regularization parameter affects the generalizability of the model.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an inputs:\n",
    "#   X_training_values - a vector of training inputs\n",
    "#   y_training_values - a vector of training outputs\n",
    "#   X_testing_values - a vector of testing inputs\n",
    "#   y_testing_values - a vector of testing outputs\n",
    "#   n - the degree of the polynomial to fit\n",
    "#   lam - the regularization parameter\n",
    "# Return:\n",
    "#   y_predicted_training_error - the training error\n",
    "#   y_predicted_testing_error - the testing error\n",
    "def polynomial_regression_error_regularized(X_training_values, y_training_values, X_testing_values, y_testing_values, n, lam):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    return y_predicted_training_error, y_predicted_testing_error\n",
    "\n",
    "training_error = []\n",
    "testing_error = []\n",
    "lam_values = [0, 0.01, 0.1, 1, 5, 10]\n",
    "for lam in lam_values:\n",
    "    y_predicted_training_error, y_predicted_testing_error = polynomial_regression_error_regularized(X_sample, y_sample, X_sample_testing, y_sample_testing, 12, lam)\n",
    "    training_error.append(y_predicted_training_error)\n",
    "    testing_error.append(y_predicted_testing_error)\n",
    "\n",
    "# Plot the training and testing error\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with larger values for the regularization paramater $\\lambda$, the testing error decreases while the training error increases. This is because the model is overfitting the training data, and the regularization term is penalizing the model for having large coefficients. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
